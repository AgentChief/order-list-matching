Below is a concrete way to **pivot the spec we just wrote** so it delivers exactly what you’re describing:

*one wide “movement” table that contains both orders and shipments, keeps the match metadata, and lets Power BI calculate the open-order book for **any past date**.*

I’ve included (a) the minimal schema, (b) how the existing reconciliation flow changes, (c) two ways to get the point-in-time view, and (d) a few clarifying questions at the bottom.

---

## 1 · Core idea

```
fact_order_movements   (append-only, event grain)
┌───────────────────────────────┬──────────────┬──────────────┬─────────┐
│ movement_id  (PK, bigint)     │ customer_id  │ match_group  │ …dims… │
│ movement_type  ('ORDER'|'SHIP')│ order_id     │ shipment_id  │        │
│ movement_date                │ qty           │ match_flag   │        │
└───────────────────────────────┴──────────────┴──────────────┴─────────┘
```

* **One row per “movement”** – a positive quantity for an order placement, a negative quantity for its shipment.
* `match_group` ties the order and its shipment(s) together (generated by the reconciliation step).
* **Never UPDATE** this table. When an order is cancelled we write a movement with `movement_type='ORDER_CANCEL'` and a negative quantity.

From this, “open order book **as at any date**” is simply the cumulative sum of quantity up to that date, filtered on movement flags.

---

## 2 · Minimal DDL

```sql
CREATE TABLE dbo.fact_order_movements (
    movement_id      BIGINT        IDENTITY(1,1) PRIMARY KEY,
    customer_id      INT           NOT NULL,
    aag_order_no     NVARCHAR(64)  NULL,
    style_norm       NVARCHAR(50)  NOT NULL,
    colour_norm      NVARCHAR(50)  NOT NULL,
    po_norm          NVARCHAR(50)  NULL,
    delivery_method_norm NVARCHAR(20) NULL,
    movement_type    VARCHAR(20)   NOT NULL,   -- ORDER | SHIP | ORDER_CANCEL
    movement_date    DATE          NOT NULL,   -- order date or ship date
    qty              INT           NOT NULL,
    match_group      UNIQUEIDENTIFIER NULL,    -- same GUID ties rows together
    match_flag       VARCHAR(20)   NULL,       -- EXACT_OK / HI_CONF …
    load_ts          DATETIME2(3)  DEFAULT SYSUTCDATETIME()
);

-- Index for cumulative sums
CREATE INDEX IX_movements_customer_date ON dbo.fact_order_movements
(customer_id, movement_date);
```

---

## 3 · How the reconciliation flow changes

| Step                      | Before (update-in-place)                                 | **Now (append)**                                                                                                                                                                         |
| ------------------------- | -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. `recon.py`             | Produced `recon_results_tmp` table with `match_flag`.    | *Same.*                                                                                                                                                                                  |
| 2. `apply_deductions.sql` | `UPDATE` quantities in `orders_current` / `mfg_current`. | **INSERT two sets of rows into `fact_order_movements`:**<br>• unmatched orders → one positive `ORDER` row<br>• matched shipments → one negative `SHIP` row with same `match_group` GUID. |
| 3. Power BI model         | Pulled from wide “current” tables                        | Pulls from **fact\_order\_movements** (plus dimension tables for date, cust, style, colour).                                                                                             |

Nothing else needs to change in the alias or HITL logic; we’re only altering the final materialisation step.

---

### Skeleton of the new SQL load

```sql
DECLARE @now DATETIME2(3) = SYSUTCDATETIME();
DECLARE @batch UNIQUEIDENTIFIER = NEWID();

-- A. Orders (positive qty) --------------------------------------------
INSERT INTO dbo.fact_order_movements
      (customer_id, aag_order_no, style_norm, colour_norm,
       po_norm, delivery_method_norm,
       movement_type, movement_date, qty,
       match_group, match_flag, load_ts)
SELECT o.customer_id, o.aag_order_no, o.style_norm, o.colour_norm,
       o.po_norm, o.delivery_method_norm,
       'ORDER', o.order_date, o.qty_order,
       rr.match_group, rr.match_flag, @now
FROM dbo.orders_current o
LEFT JOIN recon_results_tmp rr
       ON rr.order_id = o.order_id;

-- B. Shipments (negative qty) -----------------------------------------
INSERT INTO dbo.fact_order_movements
      (customer_id, aag_order_no, style_norm, colour_norm,
       po_norm, delivery_method_norm,
       movement_type, movement_date, qty,
       match_group, match_flag, load_ts)
SELECT s.customer_id, s.aag_order_no, s.style_norm, s.colour_norm,
       s.po_norm, s.delivery_method_norm,
       'SHIP', s.ship_date, -s.qty_ship,
       rr.match_group, rr.match_flag, @now
FROM dbo.shipments_current s
LEFT JOIN recon_results_tmp rr
       ON rr.shipment_id = s.shipment_id;
```

*(If an order is fully cancelled later, write an `ORDER_CANCEL` with -qty.)*

---

## 4 · Point-in-time open order book – two flavours

### 4.1 Dynamic DAX (no extra storage)

```DAX
OpenQty :=
VAR selDate = MAX( 'Date'[Date] )
RETURN
    CALCULATE (
        SUM ( fact_order_movements[qty] ),
        FILTER ( ALL ( 'Date' ), 'Date'[Date] <= selDate ),
        fact_order_movements[match_flag] IN { "EXACT_OK", "HI_CONF" }   -- exclude LOW_CONF if you like
    )
```

Pros: storage-light, easy to slice by any dimension.
Cons: if the fact table hits tens of millions of rows the cumulative filter can get slow (workable with columnstore + incremental refresh).

### 4.2 Pre-calculated daily snapshot

```sql
CREATE TABLE dbo.fact_open_orders_daily (
    snapshot_date DATE     NOT NULL,
    customer_id   INT      NOT NULL,
    style_norm    NVARCHAR(50),
    colour_norm   NVARCHAR(50),
    open_qty      INT      NOT NULL,
    PRIMARY KEY (snapshot_date, customer_id, style_norm, colour_norm)
);

-- Kestra nightly:
INSERT INTO dbo.fact_open_orders_daily
SELECT
   d::date                              AS snapshot_date,
   customer_id,
   style_norm,
   colour_norm,
   SUM(qty)                             AS open_qty
FROM dbo.fact_order_movements
CROSS JOIN (SELECT CAST(@yesterday AS DATE) d) as x
WHERE movement_date <= d
GROUP BY customer_id, style_norm, colour_norm;
```

Pros: lightning-fast in Power BI; can purge raw facts if size explodes.
Cons: one more ETL task; storage grows (`rows × days`).

---

## 5 · What changes in the repo & flow

1. **Add** `sql/10_deduction/load_fact_movements.sql` (replaces previous update script).
2. **Kestra flow**:

   ```yaml
   - id: load_fact_movements
     type: io.kestra.plugin.jdbc.Execute
     sql: sql/10_deduction/load_fact_movements.sql
   ```
3. **Power BI model**:

   * fact table = `dbo.fact_order_movements` (or `fact_open_orders_daily` if snapshot mode).
   * date, customer, item dims unchanged.
4. **Phase-1 snapshot task** (if you opt for the daily table) is now *optional*; you already have event-sourcing.

---

## 6 · Clarifying questions

1. **Partial shipments** – do you ever ship more than was ordered (overship) or partial cancellations?
2. **Order edits** – when a customer simply *changes qty* before shipping, does the Excel feed send a negative + positive (which is perfect for events) or overwrite the line?
3. **Returns / RMAs** – should a return be another negative movement?
4. **Retention** – do we need to keep raw order/ship rows forever, or can we archive after, say, 24 months?
5. **Performance target** – expected max fact table size (rows / day) so we decide if columnstore or partitioning is needed immediately.

Let me know the answers and I’ll finalise the ETL script (especially for cancellations / overships) and include an incremental-refresh template for the Power BI model.
